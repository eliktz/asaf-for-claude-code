# ASAF Implementation Feedback Command

**Command**: `/asaf-impl-feedback`

**Purpose**: Handle post-implementation changes with structured quality gates

**When to use**: After `/asaf-impl` completes and you have feedback, fixes, or improvements to apply

---

## Step 0: Verify Active Sprint

1. Check if /asaf/.current-sprint.json exists
   - If NO: Run auto-selection algorithm (see asaf-core.md)
   - If YES: Read sprint name from file

2. Validate selected sprint exists at /asaf/<sprint-name>/
   - If NO: Sprint was deleted
     - Delete stale /asaf/.current-sprint.json
     - Log: "Selected sprint no longer exists, auto-selecting..."
     - Run auto-selection algorithm
   - If YES: Continue

3. Validate sprint has .state.json
   - Check /asaf/<sprint-name>/.state.json exists
   - If NO but sprint folder exists:
     - LENIENT WARNING: Log "Sprint has no .state.json (may be incomplete)"
     - Continue anyway (developer may be fixing)
   - If sprint folder missing: Already handled in step 2

4. Set context: All subsequent operations use /asaf/<sprint-name>/

---

## Prerequisites

### Check Sprint State

Read `.state.json`:
- phase must be "implementation"
- OR phase must be "demo" (feedback after demo)
- OR phase must be "retro" (final feedback)

**If implementation not started**:
```
ğŸ”´ ERROR: Implementation not started

Current phase: [phase]

You must run /asaf-impl before collecting feedback.

Current sprint: [sprint-name]
Phase: [phase]

Options:
  /asaf-impl          - Start implementation
  /asaf-status        - Check sprint status
```
**STOP execution.**

**If implementation not complete yet**:
```
âš ï¸ WARNING: Implementation in progress

/asaf-impl is currently running (Task [X]/[N]).

You can still provide feedback, but it's recommended to wait until
implementation completes for a full review.

Continue with feedback now? (y/n)
```

---

## Opening Message

Show context as text:
```
ğŸ“ Post-Implementation Feedback

Implementation is complete. Let's collect and apply any changes needed.

I'll help you:
1. Review the implementation systematically (optional)
2. Collect all feedback in one place
3. Apply changes with quality checks (executor â†’ reviewer loop)
4. Update documentation

This ensures feedback changes maintain the same quality as /asaf-impl.
```

**USE the AskUserQuestion tool** for mode selection:

```yaml
AskUserQuestion:
  questions:
    - question: "How would you like to provide feedback?"
      header: "Mode"
      multiSelect: false
      options:
        - label: "Interactive Review"
          description: "I'll walk through each task (systematic, thorough)"
        - label: "Bulk Feedback"
          description: "You provide all feedback now (fast, if already reviewed)"
        - label: "Specific Changes"
          description: "You have exact changes to make (fastest, expert users)"
```

**Map response**:
- "Interactive Review" â†’ Mode 1
- "Bulk Feedback" â†’ Mode 2
- "Specific Changes" â†’ Mode 3

---

## Mode 1: Interactive Review

### Step 1: Task-by-Task Review

```
Let's review the implementation task by task.

[Read implementation/tasks.md to get task list]
[Read implementation/progress.md to get completion status]

Total tasks: [N]
Completed: [X]
```

**For each completed task**:

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Task [N]: [Task Name]

**Complexity**: [X] story points
**Status**: âœ… Complete
**Iterations**: [Y] (max: [Z])

**Files Modified**:
- [file1] ([lines changed])
- [file2] ([lines changed])

**Edge Cases Handled**:
[List from progress.md]

**Tests**:
âœ… [X] tests passing
[If any failures]: âš ï¸ [Y] tests failing

**Reviewer Notes** (from last iteration):
[Last reviewer feedback from progress.md]

---

Any feedback on this task?
(Type your comments, or press Enter to skip to next task)

You: [user input]
```

**Collect all feedback** before executing.

After reviewing all tasks:

```
Review complete!

Collected [N] feedback items across [X] tasks.

Ready to proceed with applying feedback? (y/n)
```

---

## Mode 2: Bulk Feedback

```
Please provide all your feedback (one item per line or paragraph):

Format suggestions:
- "Task 1: [feedback]"
- "[file.ts]: [feedback]"
- "[General]: [feedback]"

You can paste from your notes, GitHub comments, etc.

Enter your feedback (type 'done' on a new line when finished):
```

**Collect all input until user types "done".**

---

## Mode 3: Specific Changes

```
Please provide specific changes you want to make:

Format:
- "Fix [issue] in [file]"
- "Add [feature] to [component]"
- "Refactor [code] for [reason]"

You: [user provides list]
```

---

## Categorize Feedback

After collecting feedback (any mode):

```
Analyzing feedback...

[Parse each feedback item]
[Determine severity based on keywords: "bug", "broken", "error" = Bug]
[Determine severity based on keywords: "improve", "better", "refactor" = Improvement]
[Determine severity based on keywords: "add", "would be nice", "consider" = Enhancement]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Feedback Summary:

ğŸ”´ BUGS (must fix):
1. [Task X]: [Description]
2. [Task Y]: [Description]
Total: [N] bugs

ğŸŸ¡ IMPROVEMENTS (should fix):
1. [Task X]: [Description]
2. [Task Y]: [Description]
Total: [N] improvements

ğŸŸ¢ ENHANCEMENTS (nice to have):
1. [Task X]: [Description]
Total: [N] enhancements

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Total feedback items: [N]
```

**USE the AskUserQuestion tool** for scope selection:

```yaml
AskUserQuestion:
  questions:
    - question: "Which items should I address?"
      header: "Scope"
      multiSelect: false
      options:
        - label: "Fix all"
          description: "Bugs + improvements + enhancements"
        - label: "Bugs + improvements"
          description: "Skip nice-to-have enhancements"
        - label: "Bugs only"
          description: "Critical issues only"
        - label: "Custom selection"
          description: "I'll choose specific items"
```

**If "Custom selection"**, present multi-select:

```yaml
AskUserQuestion:
  questions:
    - question: "Which specific items should I fix?"
      header: "Items"
      multiSelect: true
      options:
        - label: "[Item 1 description]"
          description: "[Category: Bug/Improvement/Enhancement]"
        - label: "[Item 2 description]"
          description: "[Category]"
        # ... dynamically generated from feedback list
```

---

## Execute Feedback with Quality Gates

For each selected feedback item:

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Feedback Item [N]/[Total]: [Description]

**Category**: [Bug/Improvement/Enhancement]
**Related Task**: Task [X]
**Files Affected**: [file list]

---

[EXECUTOR PHASE]

Adopting executor persona from implementation/decisions.md...

[Read relevant task from tasks.md]
[Read design from grooming/design.md]
[Read edge cases from grooming/edge-cases.md]

Analyzing feedback: "[feedback description]"

[Implement the change]

Changes made:
- [file1]: [description of change]
- [file2]: [description of change]

---

[TEST PHASE]

Running tests...

[Run test command based on tech stack]

Test results:
âœ… [X] tests passing
[If new tests needed]: Added [Y] new tests for this change

---

[REVIEWER PHASE]

Adopting reviewer persona from implementation/decisions.md...

Quality checks:
âœ… Aligns with design.md? [Yes/No - explanation]
âœ… Edge case coverage maintained? [Yes/No - check against edge-cases.md]
âœ… Test coverage sufficient? [Yes/No - percentage]
âœ… Code quality standards met? [Yes/No]
âœ… No breaking changes? [Yes/No]

[If any checks fail]
âŒ ISSUES FOUND:
- [Issue 1]
- [Issue 2]

[Executor addresses issues]

[Re-run quality checks]

âœ… All quality checks passed.

---

Feedback item [N] complete and approved.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

**Continue for all feedback items.**

---

## Update Documentation

After all feedback items processed:

```
All feedback items completed!

Updating documentation...

[Update implementation/progress.md]

Add new section:
```

### Feedback Iteration

**Started**: [timestamp]
**Total Items**: [N]

#### Feedback Item 1: [Description]
**Category**: [Bug/Improvement/Enhancement]
**Related Task**: Task [X]
**Status**: âœ… Complete

**Changes**:
- [file]: [change description]

**Tests**:
- [New test added]: [test name]
- [Tests updated]: [count]

**Reviewer Notes**:
âœ… Aligns with design
âœ… Edge case coverage maintained
âœ… Test coverage: [percentage]

---

[Repeat for each feedback item]

---

**Feedback Summary**:
- Total items: [N]
- Bugs fixed: [X]
- Improvements made: [Y]
- Enhancements added: [Z]
- Files modified: [count]
- Tests added/updated: [count]

All tests passing: âœ…

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

### Update SUMMARY.md

Add feedback section:

```markdown
## ğŸ”„ Feedback Iterations

**Round 1** (Completed [date]):
- Items addressed: [N] ([X] bugs, [Y] improvements, [Z] enhancements)
- Files modified: [count]
- Quality maintained: âœ…

[See implementation/progress.md for details]
```

---

### Update .state.json

```json
{
  "sprint": "[sprint-name]",
  "phase": "[current-phase]",
  "status": "[current-status]",
  "feedback_rounds": [
    {
      "round": 1,
      "completed_at": "[timestamp]",
      "items_addressed": [N],
      "bugs": [X],
      "improvements": [Y],
      "enhancements": [Z]
    }
  ],
  "last_feedback_at": "[timestamp]"
}
```

---

## Success Message

```
âœ… Feedback iteration complete!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š Summary:

**Feedback Items Processed**: [N]
- ğŸ”´ Bugs fixed: [X]
- ğŸŸ¡ Improvements made: [Y]
- ğŸŸ¢ Enhancements added: [Z]

**Changes Made**:
- Files modified: [count]
- Tests added: [count]
- Tests updated: [count]

**Quality**:
âœ… All changes passed reviewer quality gates
âœ… Edge case coverage maintained
âœ… Test coverage: [percentage]
âœ… All tests passing

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Documentation Updated**:
- implementation/progress.md (feedback section added)
- SUMMARY.md (feedback round logged)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Next Steps**:

Have more feedback?
  /asaf-impl-feedback        - Run another feedback round

Ready for demo?
  /asaf-demo                 - Generate presentation

Sprint complete?
  /asaf-retro                - Run retrospective
```

---

## Multiple Feedback Rounds

Users can run `/asaf-impl-feedback` multiple times:

```
ğŸ“ Post-Implementation Feedback (Round 2)

Previous feedback rounds:
- Round 1 ([date]): [N] items addressed

This will be feedback round 2.

[Continue with normal flow]
```

Track in progress.md:

```markdown
### Feedback Iteration - Round 2

[Same structure as Round 1]
```

---

## Error Handling

### No Implementation to Provide Feedback On

```
ğŸ”´ ERROR: No implementation to provide feedback on

Current sprint: [sprint-name]
Phase: [phase]

Implementation hasn't been completed yet.

Options:
  /asaf-impl       - Start/continue implementation
  /asaf-status     - Check current status
```

---

### Test Failures During Feedback

If tests fail after applying feedback:

```
âš ï¸ WARNING: Tests failing after feedback change

Feedback item [N]: [Description]

Test failures:
- [test1]: [error]
- [test2]: [error]

The executor will attempt to fix these issues.

[Executor analyzes failures and fixes]

[Re-run tests]

[If still failing after 3 attempts]

ğŸ”´ BLOCKED: Cannot complete feedback item

Feedback item [N] could not be completed due to test failures.
```

**USE the AskUserQuestion tool**:

```yaml
AskUserQuestion:
  questions:
    - question: "How should we proceed with this blocked item?"
      header: "Action"
      multiSelect: false
      options:
        - label: "Skip and continue"
          description: "Skip this item, proceed with other feedback"
        - label: "Stop iteration"
          description: "Manual intervention needed, pause here"
        - label: "Mark incomplete"
          description: "Mark as incomplete, continue with others"
```

---

### Reviewer Rejects Changes

If reviewer rejects feedback changes:

```
âŒ REVIEW FAILED

Feedback item [N]: [Description]

Reviewer found issues:
- [Issue 1]
- [Issue 2]

The executor will address these issues.

[Executor iteration]

[Re-review]

[If fails again]

âš ï¸ WARNING: Feedback item requires multiple iterations

This feedback change is more complex than expected.

Continue iterating? (max 3 iterations per item) (y/n)
```

---

### No Feedback Provided

If user provides empty feedback:

```
No feedback provided.

It looks like you don't have any changes to make right now.

Implementation is complete and approved!

Options:
  /asaf-demo       - Generate demo presentation
  /asaf-retro      - Run retrospective when ready
```

---

## Design Notes

### Why This Command Exists

**Problem**: After `/asaf-impl`, users provide feedback in ad-hoc conversations. Changes are made without:
- Reviewer oversight
- Edge case validation
- Test updates
- Documentation

Result: Quality degrades.

**Solution**: Structured feedback with same rigor as implementation.

### Key Principles

1. **Quality Gates**: Every feedback change goes through executor â†’ test â†’ reviewer
2. **Documentation**: All feedback tracked in progress.md
3. **Categorization**: Bugs vs improvements vs enhancements (prioritization)
4. **Multiple Rounds**: Can run feedback multiple times
5. **Flexible Input**: Interactive review, bulk feedback, or specific changes

### Comparison to /asaf-impl

| Aspect | /asaf-impl | /asaf-impl-feedback |
|--------|-----------|---------------------|
| **Input** | tasks.md (planned) | User feedback (reactive) |
| **Scope** | All planned tasks | Specific changes |
| **Iterations** | Max per task | Max per feedback item |
| **Quality** | High (executor â†’ reviewer) | High (same loop) |
| **Documentation** | progress.md | progress.md (feedback section) |

Both maintain ASAF quality standards.

---

## Future Enhancements (Not in v1)

- Auto-review: Claude suggests improvements proactively
- Feedback templates: Common feedback patterns (performance, error messages, etc.)
- Diff viewer: Show before/after for each change
- Undo: Rollback specific feedback changes
- Batch export: Export all feedback as GitHub issues

---

_This command ensures post-implementation quality matches implementation quality._
